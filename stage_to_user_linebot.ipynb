{"cells":[{"cell_type":"markdown","source":["# Paths\n* `/mnt/stage-zone/`: Stage zone\n* `/mnt/user-linebot/`: User zone for LINE bot data\n* `/mnt/code/`: Code bucket (unused)\n\n**NOTE**: when accessing path via Python, append `/dbfs` to the beggining of the path. Example: `/dbfs/mnt/stage-zone/`"],"metadata":{}},{"cell_type":"code","source":["# Imports\nimport os\nimport glob\nimport logging\nfrom pyspark.sql.types import *\nfrom pyspark.sql import Row\nfrom azure.cognitiveservices.language.textanalytics import TextAnalyticsClient\nfrom msrest.authentication import CognitiveServicesCredentials\nfrom sendgrid import SendGridAPIClient\nfrom sendgrid.helpers.mail import Mail\n\n# Set schema\nlinebot_schema = StructType([\n  StructField('message_id', LongType(), True),\n  StructField('message_type', StringType(), True),\n  StructField('message_text', StringType(), True),\n  StructField('timestamp', LongType(), True),\n  StructField('type', StringType(), True),\n  StructField('replyToken', StringType(), True),\n  StructField('source_userid', StringType(), True),\n  StructField('source_type', StringType(), True),\n  StructField('mode', StringType(), True),\n  StructField('published_timestamp', TimestampType(), True),\n  StructField('loaded_timestamp', TimestampType(), True)\n])\n\n# Variables for Text Analytics API\nsubscription_key = os.getenv('TEXT_ANALYTICS_API_KEY')\nendpoint = os.getenv('TEXT_ANALYTICS_ENDPOINT')"],"metadata":{},"outputs":[],"execution_count":2},{"cell_type":"code","source":["# Functions\ndef authenticateClient():\n  \"\"\"Authenticate the Text Analytics API client with subscription key and set endpoint\"\"\"\n  credentials = CognitiveServicesCredentials(subscription_key)\n  text_analytics_client = TextAnalyticsClient(endpoint=endpoint, credentials=credentials)\n  return text_analytics_client\n\ndef language_detection(df):\n  \"\"\"Detect language of a message\"\"\"\n  client = authenticateClient()\n  # Prepare list of documents for Text Analytics API\n  documents = []\n  processed_documents = []\n  for row in df.collect():\n    documents.append({\n      'id': row['message_id'],\n      'text': row['message_text']\n    })\n\n  response = client.detect_language(documents=documents)\n\n  for doc in response.documents:\n    processed_documents.append({\n      'id': doc.id,\n      'language': doc.detected_languages[0].name,\n      'language_short': doc.detected_languages[0].iso6391_name,\n      'language_score': doc.detected_languages[0].score\n    })\n\n  # Create processed documents data frame and merge with initial one\n  processed_docs_df = spark.createDataFrame(Row(**x) for x in processed_documents).withColumnRenamed('id', 'message_id')\n  merged_df = df.join(processed_docs_df, on=['message_id'])\n\n  # Clean memory\n  del documents\n  del processed_documents\n  del processed_docs_df\n\n  return merged_df\n\n  \ndef sentiment(df):\n  \"\"\"Get text sentiment (Positive/Neutral/Negative) and it's score.\n  Score is between 0 and 1. Text sentiment is classified as following:\n  Positive: >= 0.75\n  Neutral: >= 0.25 and < 0.75\n  Negative: < 0.25\n  \"\"\"\n  client = authenticateClient()\n  # Prepare list of documents for Text Analytics API\n  documents = []\n  processed_documents = []\n  for row in df.collect():\n    documents.append({\n      'id': row['message_id'],\n      'language': row['language_short'],\n      'text': row['message_text']\n    })\n\n  response = client.sentiment(documents=documents)\n\n  for doc in response.documents:\n    sentiment_category = 'Positive' if doc.score >= 0.75 else ('Neutral' if doc.score >= 0.25 else 'Negative')\n    processed_documents.append({\n      'id': doc.id,\n      'sentiment': sentiment_category,\n      'sentiment_score': float(doc.score)\n    })\n\n  # Create processed documents data frame and merge with initial one\n  processed_docs_df = spark.createDataFrame(Row(**x) for x in processed_documents).withColumnRenamed('id', 'message_id')\n  merged_df = df.join(processed_docs_df, on=['message_id'])\n\n  # Clean memory\n  del documents\n  del processed_documents\n  del processed_docs_df\n\n  return merged_df\n        \n\ndef transform(dbfs_path, local_path):\n  \"\"\"ETL flow: read csv data, transform (limit columns), export to user zone\n        dbfs_path: full path to .csv file with /dbfs prefix\n        local_path: mounted path starting with /mnt\"\"\"\n  # Extract\n  stage_df = spark.read.csv(local_path, header=True, schema=linebot_schema)\n  # Transform\n  limited_df = stage_df.select('message_id', 'source_userid', 'message_text', 'published_timestamp', 'loaded_timestamp')\n  limited_df = language_detection(limited_df)\n  limited_df = sentiment(limited_df)\n  # Load\n  dbfs_user_path = dbfs_path.replace(\"stage-zone\", \"user-linebot\")\n  # Make directories, otherwise raises exception when exporting\n  os.makedirs(dbfs_user_path.replace(dbfs_user_path.split('/')[-1], ''), exist_ok=True)\n  limited_df.toPandas().to_csv(dbfs_user_path, header=True, index=False, quoting=1)\n  \n\ndef send_email(subject, message):\n  \"\"\"Send email message via SendGrid service to my private email\"\"\"\n  msg_to_send = Mail(\n    from_email='kirill@stereowind.com',\n    to_emails='stereowind@gmail.com',\n    subject=subject,\n    html_content=message)\n  try:\n    sg = SendGridAPIClient(os.getenv('SENDGRID_API_KEY'))\n    response = sg.send(msg_to_send)\n    print(\"Email notification sent!\")\n  except Exception as e:\n    print(\"Error occured while trying to send an email notification:\")\n    print(str(e))"],"metadata":{},"outputs":[],"execution_count":3},{"cell_type":"code","source":["# Main\n\n# Get all csv file paths in stage zone, paths in processed_files table and subtract from them\n#   all_df: list with all .csv files currently in stage zone\n#   processed_files: previously processed files (already exist in user-linebot storage)\n#   to_process: files that are going to be processed in this session \nall_files = glob.glob(\"/dbfs/mnt/stage-zone/\" + \"**/*.csv\", recursive=True)\nall_df = spark.createDataFrame(all_files, StringType()).withColumnRenamed('value', 'processed_files')\nprocessed_files = spark.table('datalake.processed_files_linebot')\nto_process = all_df.subtract(processed_files)\n\n# Transform data\nnum_files = to_process.count()\nif num_files > 0:\n  for row in to_process.collect():\n    dbfs_path = row['processed_files']\n    local_path = dbfs_path.replace('/dbfs', '')\n    try:\n      transform(dbfs_path, local_path)\n      spark.sql(f\"INSERT INTO datalake.processed_files_linebot VALUES ('{dbfs_path}')\")\n    except Exception as e:\n      print(f\"File {localpath} failed to get processed!\")\n\n# Send email notification\nsend_email(\"Azure Linebot Datalake processing completed\", f\"Files processed: <strong>{num_files}</strong>\")\nprint(\"Processing finished.\")"],"metadata":{},"outputs":[],"execution_count":4}],"metadata":{"name":"stage_to_user_linebot","notebookId":2947308661992721},"nbformat":4,"nbformat_minor":0}
